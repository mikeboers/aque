- Broker.capture -> Broker.acquire

- Broker._emit_event(type, *args, **kwargs) and Broker._dispatch_event(type, *args, **kwargs)
    - immediately dispatches them locally (to self._handle_<event>)
    - uses postgres' notify feature
    - ignores events that come back to the same PID

- worker could be written so that it has an event loop instead of having
  a ton of threads. Scheduling new jobs could be during idle.

  job = (ThreadJob or ProcJob)(self, task)
  job.start()
  self._eventloop.add(job)

  ThreadJob for non-forking brokers
    - has a selectable event to signal that it is done
    - replaces sys.stdout and sys.stderr so that they can be captured
  ProcJob for forking brokers
    - has selectable event to signal that it is done
    - can capture IO, and exposes that

  EventLoop class handles all of these.

    loop.add(selectable)
    loop.remove(selectable)

    loop.run_one(timeout=0) -> process one
    loop.run_to_end(timeout=None)
    loop.run_forever() -> when it runs out of selectables it waits for something
      new to be added. This can be a global loop which runs for the benifit of
      all brokers that are not attached to workers.

    loop.stop() stops the loop if within a thread

  Make a Selectable interface

    job.to_select() -> (), (), ()
      - raise StopSelection to be removed from the EventLoop
    job.on_select(rfds, wfds, xfds)
      - these may all be empty, indicating idle

  should have a reasonable timeout so that the jobs are able to detect if they
  have finished running without using the selectable interface (just in case)
  it fails
  
  Use this for:
    - IO for running tasks
    - waiting for running tasks to finish
    - stoppers
    - scheduling new tasks
    - broker IO (this isn't actually a great idea since it needs to run on its
      own, OR broker.set_async_pool(worker.async_pool) can have it be taken over
      by the worker

- write a test that makes sure that multiple workers run a given task
  only once

- run as actual user (if root)

- deploy onto main servers (via AQUE_BROKER="$(ficonfig get aque_broker)")

- `aque worker --reload` uses metatools reloader (or change detection and
  a restart via os.exec-ing back to itself)

- `killall -HUP aque-worker` -> it execs to itself
  - can I set it's name to aque-worker after it has started, or should I
    actually make various aque-* commands?

- brokers should signal when there are new tasks so that the workers can
  fight over them immediately

- `aque kill <tid>` -> sends an event that the worker hears, and then kills
  the subprocess

- broker should have a standard event listening interface. Take a look at the
  blinker API and see if we should replicate that with Postgres.

- aque xargs only needs to support -P (for parallelism) and -L (for lines)

  - "-P" should set the CPU reservation to "/<value>" (or similar) to indicate
    that we want to reserve a fraction of the total. Should do it this way
    instead of on the submitter because the submitter may not have a matching
    number of processors.

- add stdin, stdout, stderr, but should I also have stdin_value,
  somehow encode that into the same field, or just don't have that capability?

- Document (in hacking notes) that I should have the task be relatively flat, and keep the
  pickled things as seperate from each other as possible. We will build
  more flexibility in as required by making the pattern into a class and
  having a bunch of hooks on it for various things.

  Keeping the pickled things seperate allows us to use the least invasive
  pickling that is possible, and keep depickling errors as isolated as possible.

  At most I will consider having an "extra" field which is everything left
  over that gets pickled.

- `aque status --watch` for top-like output

- enumerate all of the states:
    - creating
    - pending
    - cancelled
    - running (I don't know if this would actually exist though)
    - success
    - error

    - creqte aque.states with sets of the above:
        READY_STATES
        UNREADY_STATES
        COMPLETE_STATES

- shell patterns should automatically add IO hints for any arguments that appear to be paths
    - add the longest subpath which exists (even just '/') to the set
    - sum up the size of fully-specified files for a pseudo "duration"

  SOLUTION: only allow functions/classes that can be resolved by name, and
  encode them into our format first

- task['interpreter'] should be able to set which interpreter we use
  (which is why qbfutures has sandbox.the_corner to work in)
    - need Broker.to_url to pass via AQUE_BROKER

- aque-xargs should match as much of xargs as we need
- aque-kill should kill a task
    - `aque-kill -r` would kill all dependencies

- send "task_status" -> "running" when a task starts executing
    or a task_heartbeat with the ID

- workers should respond to priority hints:
    - e.g. hints={'paths': [...], 'duration': 100}
    - 'io_hints' is a sequence of paths that may have IO
    - 'rel_duration' is a float relative to other durations of the same priority,
       and we prioritize longer processes of the same priority

- reconsider adding children to the graph. They could be exactly the same as
  dependencies, except that they inherit from their parent.

  - children should not be under their parent as far as priority is concerned
    as that will introduce inefficiencies for our IO prioritization

  - it seems the tough problem is mostly determining how to present tasks
    in the Web UI.

- worker should be able to schedule a listener on the broker for when there
  are new tasks so that it can immediately wake up

    broker.register_worker(self)
        - store it in a weakref dict
    broker will then call worker.wake_for_work() when there is new pending work

- document relationships of queues, brokers, task prototypes, and futures.
