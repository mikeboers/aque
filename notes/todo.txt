- `aque status` should merge everything into a heirarchy
  - --flat prevents this
  - --depth 0 describes how far to go down the heirarchy

- move signal/kill api to the broker
  broker.signal(tids, signal)

- tasks should enforce host requirements

- logging in subprocesses should be put through to a special pipe, or perhaps
  through a socket of some sort
  - the logs should be attributes of workers, jobs, queues, brokers, etc, and
    those ones should be modified so they pipe back to their counterparts
    and then don't go further up the chain. This will allow existing logging
    to actually work

- preserve environment; stuff os.environ into task['environ'] ?
  - `aque submit KEY=value blah blah` env style

- is there a way of detecting what the mount point is for various io_paths and
  making sure that it corresponds?

- write a test that makes sure that multiple workers run a given task
  only once
    change what the timeouts are heartbeats are for this test so that we
    can simulate

- deploy onto main servers (via AQUE_BROKER="$(ficonfig get aque_broker)")
  - fix permissions; everyone is --superuser so far
  âˆš hk
  - nt
  - ex

- `aque worker --reload` uses metatools reloader (or change detection and
  a restart via os.exec-ing back to itself)

- `killall -HUP aque-worker` -> it execs to itself
  - can I set it's name to aque-worker after it has started, or should I
    actually make various aque-* commands?

- worker should listen to "task_status.pending" to pick up new tasks
  - make it take over the event loop, and just bind something meaningless to
    task_status.pending

- add stdin, stdout, stderr, but should I also have stdin_value,
  somehow encode that into the same field, or just don't have that capability?

- Document (in hacking notes) that I should have the task be relatively flat, and keep the
  pickled things as seperate from each other as possible. We will build
  more flexibility in as required by making the pattern into a class and
  having a bunch of hooks on it for various things.

  Keeping the pickled things seperate allows us to use the least invasive
  pickling that is possible, and keep depickling errors as isolated as possible.

  At most I will consider having an "extra" field which is everything left
  over that gets pickled.

- `aque status --watch` for top-like output

- enumerate all of the states:
    - creating
    - pending
    - cancelled
    - running (I don't know if this would actually exist though)
    - success
    - error

    - creqte aque.states with sets of the above:
        READY_STATES
        UNREADY_STATES
        COMPLETE_STATES

- shell patterns should automatically add IO hints for any arguments that appear to be paths
    - add the longest subpath which exists (even just '/') to the set
    - sum up the size of fully-specified files for a pseudo "duration"

  SOLUTION: only allow functions/classes that can be resolved by name, and
  encode them into our format first

- task['interpreter'] should be able to set which interpreter we use
  (which is why qbfutures has sandbox.the_corner to work in)
    - need Broker.to_url to pass via AQUE_BROKER

- aque-xargs should match as much of xargs as we need
- aque-kill should kill a task
    - `aque-kill -r` would kill all dependencies

- send "task_status" -> "running" when a task starts executing
    or a task_heartbeat with the ID

- workers should respond to priority hints:
    - e.g. hints={'paths': [...], 'duration': 100}
    - 'io_hints' is a sequence of paths that may have IO
    - 'rel_duration' is a float relative to other durations of the same priority,
       and we prioritize longer processes of the same priority

- reconsider adding children to the graph. They could be exactly the same as
  dependencies, except that they inherit from their parent.

  - children should not be under their parent as far as priority is concerned
    as that will introduce inefficiencies for our IO prioritization

  - it seems the tough problem is mostly determining how to present tasks
    in the Web UI.

- worker should be able to schedule a listener on the broker for when there
  are new tasks so that it can immediately wake up

    broker.register_worker(self)
        - store it in a weakref dict
    broker will then call worker.wake_for_work() when there is new pending work

- document relationships of queues, brokers, task prototypes, and futures.
