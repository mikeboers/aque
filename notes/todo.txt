- aque xargs only needs to support -P (for parallelism) and -L (for lines)

  - "-P" should set the CPU reservation to "/<value>" (or similar) to indicate
    that we want to reserve a fraction of the total. Should do it this way
    instead of on the submitter because the submitter may not have a matching
    number of processors.

- add stdin, stdout, stderr, but should I also have stdin_value,
  somehow encode that into the same field, or just don't have that capability?

- Document that I should have the task be relatively flat, and keep the
  pickled things as seperate from each other as possible. We will build
  more flexibility in as required by making the pattern into a class and
  having a bunch of hooks on it for various things.

  Keeping the pickled things seperate allows us to use the least invasive
  pickling that is possible, and keep depickling errors as isolated as possible.

  At most I will consider having an "extra" field which is everything left
  over that gets pickled.

- `aque status --watch` for top-like output

- enumerate all of the states:
    - creating
    - pending
    - cancelled
    - running (I don't know if this would actually exist though)
    - success
    - error

    - creqte aque.states with sets of the above:
        READY_STATES
        UNREADY_STATES
        COMPLETE_STATES

- shell patterns should automatically add IO hints for any arguments that appear to be paths
    - add the longest subpath which exists (even just '/') to the set
    - sum up the size of fully-specified files for a pseudo "duration"

  SOLUTION: only allow functions/classes that can be resolved by name, and
  encode them into our format first

- task['interpreter'] should be able to set which interpreter we use
  (which is why qbfutures has sandbox.the_corner to work in)
    - need Broker.to_url to pass via AQUE_BROKER

- aque-xargs should match as much of xargs as we need
- aque-kill should kill a task
    - `aque-kill -r` would kill all dependencies

- send "task_status" -> "running" when a task starts executing
    or a task_heartbeat with the ID

- workers should run more than one job at once
    - e.g. requirements={'cpu': 2, 'mem': '8MB'}
    - 'cpu' is number of cpus the task would like, defaults to 1
    - 'mem' is the amount of ram required, defaults to 0

- workers should respond to priority hints:
    - e.g. hints={'paths': [...], 'duration': 100}
    - 'io_hints' is a sequence of paths that may have IO
    - 'rel_duration' is a float relative to other durations of the same priority,
       and we prioritize longer processes of the same priority

- workers should actually capture tasks

    worker table:
        id
        hostname
        ip
        last_active -> time of last heartbeat
        
    SQL schema should include:
        captured_by -> worker.id
        last_active -> time of last heartbeat

    # This will also schedule a heartbeat.
    # By default assume a 30s timeout.
    if not broker.capture(tid):
        continue

    try:
        xxx
    finally:
        broker.release(tid)

    - broker will need to synchronize time with the server:
        '''select CURRENT_TIME''' and calculate the offset


- reconsider adding children to the graph. They could be exactly the same as
  dependencies, except that they inherit from their parent.

  - children should not be under their parent as far as priority is concerned
    as that will introduce inefficiencies for our IO prioritization

  - it seems the tough problem is mostly determining how to present tasks
    in the Web UI.

- worker should be able to schedule a listener on the broker for when there
  are new tasks so that it can immediately wake up

    broker.register_worker(self)
        - store it in a weakref dict
    broker will then call worker.wake_for_work() when there is new pending work

- document relationships of queues, brokers, task prototypes, and futures.
